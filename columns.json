{
    "output.HalluScorerEvaluator.is_hallucination_ground_truth.true_fraction" : "The proportion of test cases where the ground truth annotations indicate hallucinations. This metric helps in understanding how frequently hallucinations are labeled in the reference dataset.",
    "output.HalluScorerEvaluator.is_hallucination_ground_truth.true_count" : "The absolute number of hallucinations identified in the ground truth dataset. This represents the total occurrences of hallucinations as per human or predefined annotations.",
    "output.HalluScorerEvaluator.scorer_evaluation_metrics.precision" : "Precision score: The proportion of predicted hallucinations that are actually hallucinations. A high precision means fewer false positives, indicating the model rarely misclassifies non-hallucinations as hallucinations.",
    "output.HalluScorerEvaluator.scorer_evaluation_metrics.accuracy" : "Accuracy score: The fraction of total predictions (hallucinations and non-hallucinations) that are correct. This provides a general measure of the hallucination detector’s reliability.",
    "output.HalluScorerEvaluator.scorer_evaluation_metrics.recall" : "Recall score: The proportion of actual hallucinations that were correctly detected. A high recall score indicates that the model is good at capturing most hallucinations, reducing false negatives.",
    "output.HalluScorerEvaluator.is_hallucination.true_fraction" : "The fraction of generated outputs classified as hallucinations. This provides an estimate of how frequently hallucinations occur in the model’s responses.",
    "output.HalluScorerEvaluator.scorer_accuracy.true_fraction" : "The proportion of evaluation cases where the hallucination scorer correctly labeled hallucinations and non-hallucinations, indicating its reliability.",
    "output.HalluScorerEvaluator.scorer_evaluation_metrics.f1" : "F1 score: The harmonic mean of precision and recall, balancing the trade-off between false positives and false negatives. It is a key measure of model performance, especially when class distributions are imbalanced.",
    "output.HalluScorerEvaluator.scorer_worked.true_fraction" : "The fraction of test cases where the hallucination scorer executed successfully without errors, ensuring the system's robustness and operability.",
    "output.HalluScorerEvaluator.is_hallucination.true_count" : "The total number of outputs classified as hallucinations. This gives an absolute count of detected hallucinations in the generated responses.",
    "output.HalluScorerEvaluator.scorer_accuracy.true_count" : "The total number of cases where the hallucination scorer made correct predictions, providing insight into its effectiveness.",
    "output.HalluScorerEvaluator.scorer_worked.true_count" : "The number of instances where the hallucination scorer executed successfully without failure. A low count may indicate operational issues with the scorer.",
    "output.HalluScorerEvaluator.total_completion_tokens.mean" : "The average number of tokens in model completions evaluated by the hallucination scorer. This provides insight into the length of responses that were assessed for hallucinations.",
    "output.model_output.total_completion_tokens.mean" : "The average number of tokens in model-generated responses, indicating the typical length of outputs produced by the model.",
    "output.HalluScorerEvaluator.generation_time.mean" : "The average time taken for the hallucination evaluation process, measuring the efficiency of the scoring mechanism.",
    "output.HalluScorerEvaluator.total_tokens.mean" : "The average number of tokens processed during hallucination evaluation, encompassing both input and output tokens.",
    "output.model_output.generation_time.mean" : "The average response generation time of the model, indicating how quickly it produces outputs.",
    "output.model_output.total_tokens.mean" : "The average number of tokens processed in model-generated outputs, including both input and output tokens.",
    "attributes.lr_scheduler_type" : "The type of learning rate scheduler used during training, which influences how the learning rate is adjusted over time for optimal convergence.",
    "attributes.num_train_epochs" : "The total number of training epochs, indicating how many full passes were made over the training dataset.",
    "attributes.learning_rate" : "The learning rate used during model training, a key hyperparameter that affects model convergence speed and stability.",
    "attributes.warmup_ratio" : "The warmup ratio for learning rate scheduling, defining the fraction of training steps for which the learning rate gradually increases before stabilizing.",
    "attributes.model_name" : "The name of the trained model, which may include the model architecture and specific variant used.",
    "attributes.dataset_name" : "The name of the dataset used for training and/or evaluation, providing context on the data source.",
    "summary.weave.display_name" : "The name of the experiment as displayed in Weave, used for tracking and visualization purposes.",
    "attributes.wandb_run_name" : "The Weights & Biases (W&B) run name, which uniquely identifies the training run in the W&B dashboard.",
    "attributes.wandb_run_url" : "The URL linking to the W&B training run, allowing access to detailed experiment logs and metrics.",
    "summary.weave.status" : "The current status of the Weave experiment (e.g., running, completed, failed), indicating progress or potential issues.",
    "summary.weave.latency_ms" : "The latency recorded in Weave, measured in milliseconds, which may reflect system performance and processing delays.",
    "output.model_latency.mean" : "The average model inference latency, representing the time taken to generate responses and a key factor in real-time applications."
}
